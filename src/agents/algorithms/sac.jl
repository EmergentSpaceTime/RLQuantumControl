"""
    SACParameters(H_bar::Float32; kwargs...)

Parameter struct for SAC algorithm.

Fields (& Kwargs):
  * `H_bar`: Target entropy (usually -dim(`action_space`)).
  * `capacity`: Number of transitions stored in memory (default: `100000`).
  * `hiddens`: Dimensions of hidden layers (default: `[256, 256]`).
  * `log_var_min`: Minium log standard deviation for stable training (default:
        `-10`).
  * `log_var_max`: Maximum log standard deviation for stable training (default:
        `3`).
  * `use_tqc`: Whether using a distribution of Q-networks (default: `true`).
  * `n_q`: Number of output q-values for TQC (default: `25`).
  * `k_q`: Bottom number of quintiles to keep for TQC (default: `21`).
  * `dropout`: Dropout probability for dropout Q-networks (default: `0.01`).
  * `layer_norm`: Whether to use layer normalisation after each layer for
        Q-networks (default: `true`).
  * `gamma`: Discount parameter for extrinsic/intrinsic rewards (default:
        `0.99`).
  * `minibatch_size`: Minibatch size for policy and Q-networks (default: `256`).
  * `training_steps`: Number of training steps per episode (default: `20`).
  * `clips`: Global gradient clippings (default: `[5.0, 5.0, 5.0, 5.0]`).
  * `decays`: Weight decays (default: `[0.0, 0.0, 0.0, 0.0]`).
  * `eta`: Learning rates (default: `[3e-4, 3e-4, 3e-4, 3e-4]`).
  * `rho`: Polyak update parameter (default: `0.005`).
  * `warmup_normalisation_episodes`: Number of initial episodes for observation
        normalisation (default: `50`).
  * `warmup_evaluation_episodes`: Number of initial episodes to populate the
        buffer (default: `50`).
  * `episodes`: Number of total episodes (default: `5000`).
"""
@kwdef struct SACParameters
    H_bar::Float32
    capacity::Int = 100000
    hiddens::Vector{Int} = [256, 256]
    log_var_min::Float32 = -10
    log_var_max::Float32 = 3
    use_tqc::Bool = true
    n_q::Int = 25
    k_q::Int = 46
    dropout::Float32 = 0.01
    layer_norm::Bool = true
    gamma::Float32 = 0.99
    minibatch_size::Int = 256
    training_steps::Int = 20
    decays::Vector{Float32} = [0.0, 0.0, 0.0, 0.0]
    clips::Vector{Float32} = [5.0, 5.0, 5.0, 5.0]
    eta::Vector{Float32} = [3e-4, 3e-4, 3e-4, 3e-4, 3e-4]
    rho::Float32 = 0.005
    warmup_normalisation_episodes::Int = 50
    warmup_evaluation_episodes::Int = 50
    episodes::Int = 1000
end


struct SACNetworks{
    B <: Union{Nothing, GPT, Recurrent},
    P <: Chain,
    Q <: Chain,
    A <: AbstractArray{Float32, 0},
    T <: AbstractVector{Float32},
}
    base_layer::B
    policy_layers::P
    Q_1_layers::Q
    Q_1_target_layers::Q
    Q_2_layers::Q
    Q_2_target_layers::Q
    log_alpha::A
    action_scale::T
    action_bias::T
end

@layer SACNetworks

function trainable(n::SACNetworks)
    return (
        ;
        base_layer=n.base_layer,
        policy_layers=n.policy_layers,
        Q_1_layers=n.Q_1_layers,
        Q_1_target_layers=n.Q_1_target_layers,
        Q_2_layers=n.Q_2_layers,
        Q_2_target_layers=n.Q_2_target_layers,
        log_alpha=n.log_alpha,
    )
end

function SACNetworks(
    observation_dim::Int,
    action_dim::Int,
    action_space::Vector{ClosedInterval{Float32}};
    base_layer::String = "none",
    embedding_dim::Int = 128,
    hiddens::Vector{Int} = [256, 256],
    q_n::Int = 1,
    q_dropout::Float32 = 0.0f0,
    q_layer_norm::Bool = false,
    activation::Function = relu,
    init::Function = glorot_normal,
    rng::AbstractRNG = default_rng(),
)
    continuous = isa(action_space, Vector{ClosedInterval{Float32}})
    if base_layer == "gpt"
        base_layer = GPT(
            observation_dim,
            action_dim,
            continuous;
            embedding_dim=embedding_dim,
        )
    elseif base_layer == "recurrent"
        base_layer = Recurrent(
            observation_dim,
            action_dim,
            continuous;
            embedding_dim=embedding_dim,
            init=init,
            rng=rng,
        )
    elseif base_layer == "none"
        base_layer = nothing
    else
        throw(
            ArgumentError(
                "Invalid base layer: '$base_layer'. Must be 'gpt', 'recurrent',"
                * " or 'none'."
            )
        )
    end
    policy_layers = _create_ffn(
        base_layer == "none" ? observation_dim : embedding_dim,
        hiddens;
        out=action_dim,
        double_out=continuous,
        activation=activation,
        init=init,
        rng=rng,
    )
    Q_1_layers = _create_ffn(
        base_layer == "none" ? observation_dim + action_dim : embedding_dim,
        hiddens;
        out=q_n,
        dropout=iszero(q_dropout) ? nothing : q_dropout,
        layer_norm=q_layer_norm,
        activation=activation,
        init=init,
        rng=rng,
    )
    Q_2_layers = _create_ffn(
        base_layer == "none" ? observation_dim + action_dim : embedding_dim,
        hiddens;
        out=q_n,
        dropout=iszero(q_dropout) ? nothing : q_dropout,
        layer_norm=q_layer_norm,
        activation=activation,
        init=init,
        rng=rng,
    )
    log_alpha = zeros(Float32)

    action_scale = @. (
        (rightendpoint(action_space) - leftendpoint(action_space)) / 2
    )
    action_bias = @. (
        (rightendpoint(action_space) + leftendpoint(action_space)) / 2
    )
    return SACNetworks(
        base_layer,
        policy_layers,
        Q_1_layers,
        deepcopy(Q_1_layers),
        Q_2_layers,
        deepcopy(Q_2_layers),
        log_alpha,
        action_scale,
        action_bias,
    )
end


struct SACAgent{
    M <: ReplayBuffer,
    N <: SACNetworks,
    O <: AbstractVector,
} <: Agent{M, N, O}
    params::SACParameters
    memory::M
    networks::N
    opt_states::O
end

"""
    SACAgent(
        env::QuantumControlEnvironment,
        rng::AbstractRNG = default_rng();
        kwargs...
    )

Struct of a agent that uses the Soft Actor Critic algorithm
[haarnoja2018soft](@cite). with optional dropout Q-networks
[chen2021randomized](@cite) and TQC [kuznetsov2020controlling](@cite) to improve
sample efficiency.

Args:
  * `env`: The environment that the agent learns. This just extracts relavant
        information from the environment such as the observation and action
        spaces.
  * `rng`: Random number generator (default: [`Random.default_rng()`]()).

Kwargs:
  * `activation`: Activation function for neural networks (default: relu).
  * `init`: Initialisation function for neural networks (default:
        glorot_normal).
  * `params_kwargs`: Keyword arguments for SACParameters.

Fields:
  * `params`: Hyper parameters for the agent.
  * `memory`: Replay buffer with a history of transitions.
  * `networks`: Neural networks.
  * `opt_states`: Neural networks optimiser states.
"""
function SACAgent(
    env::QuantumControlEnvironment,
    rng::AbstractRNG = default_rng();
    activation::Function = relu,
    init::Function = glorot_normal,
    kwargs...,
)
    observation_dim = length(env.observation_space)
    action_dim = length(env.action_space)
    H_bar = -action_dim

    params = SACParameters(; H_bar=H_bar, kwargs...)
    memory = ReplayBuffer(true, observation_dim, action_dim, params.capacity)
    networks = gpu(
        SACNetworks(
            observation_dim,
            action_dim,
            env.action_space;
            params.use_tqc ? params.n_q : 1,
            hiddens=params.hiddens,
            q_dropout=params.dropout,
            q_layer_norm=params.layer_norm,
            activation=activation,
            init=init,
            rng=rng,
        )
    )
    opt_states = [
        setup(
            OptimiserChain(
                ClipNorm(params.clips[i]),
                AdamW(params.eta[i], (0.9f0, 0.999f0), params.decays[i]),
            ),
            network,
        )
        for (i, network) in [
            (1, networks.base_layer),
            (2, networks.policy_layers),
            (3, networks.Q_1_layers),
            (4, networks.Q_2_layers),
            (5, networks.logÎ±),
        ]
    ]
    return SACAgent(params, memory, networks, opt_states)
end

"""
    get_action(
        agent::SACAgent,
        observation::Vector{Float64},
        rng::AbstractRNG = default_rng(),
    )

Retrieve an action from a policy informed by the current observation.

Args:
  * `agent`: The SAC agent.
  * `observation`: The current observation.
  * `rng`: Random number generator (default: [`Random.default_rng()`]()).

Returns:
  * `Vector{Float64}`: The action to take.
"""
function get_action(
    agent::SACAgent,
    observation::Vector{Float64},
    rng::AbstractRNG = default_rng(),
)
    Î¼, log_var_u = cpu(
        agent.networks.policy_layers(gpu(f32(observation)))
    )
    log_var = @. (
        agent.params.log_var_min
        + (tanh(log_var_u) + 1)
        * (agent.params.log_var_max - agent.params.log_var_min)
        / 2
    )
    N_01 = randn(rng, Float32, length(Î¼))
    action = @. (
       tanh(Î¼ + exp(log_var / 2) * N_01) * agent.networks.action_scale
       + agent.networks.action_bias
    )
    return convert(Vector{Float64}, action)
end

function get_random_action(agent::SACAgent, rng::AbstractRNG = default_rng())
    return convert(
        Vector{Float64},
        @. (
            tanh($randn(rng, Float32, $length(agent.networks.action_scale)))
            * agent.networks.action_scale
            + agent.networks.action_bias
        )
    )
end

function evaluation_steps!(
    agent::SACAgent,
    env::QuantumControlEnvironment,
    rng::AbstractRNG = default_rng(),
)
    sum_r = zero(Float64)

    observation = reset!(env, rng)
    done = false
    while !done
        index = update_and_get_index!(agent.memory)
        agent.memory.observations_t[:, index] = observation

        action = get_action(agent, observation, rng)
        agent.memory.actions[:, index] = action

        observation, done, reward = step!(env, action)
        agent.memory.observations_tp1[:, index] = observation
        # agent.memory.rewards[index] = reward[end]
        agent.memory.rewards[index] = reward[1]
        agent.memory.dones[index] = done
        sum_r += reward[1]
    end
    return sum_r
end

function _initial_steps!(
    agent::SACAgent,
    env::QuantumControlEnvironment,
    rng::AbstractRNG = default_rng(),
)
    if (
        (env.observation_function isa NormalisedObservation)
        | (env.reward_function isa NormalisedReward)
    )
        for _ in 1:agent.params.warmup_normalisation_episodes
            _ = reset!(env, rng)
            done = false
            while !done
                _, done, _ = step!(env, get_random_action(agent, rng))
            end
        end
    end
    for _ in 1:agent.params.warmup_evaluation_episodes
        _ = evaluation_steps!(agent, env, rng)
    end
    return nothing
end

function trainer_steps!(agent::SACAgent, rng::AbstractRNG = default_rng())
    metrics = zeros(Float32, 7, agent.params.training_steps)
    for i in 1:agent.params.training_steps
        if agent.params.use_tqc
            metrics[:, i] = _update_agent_networks_tqc!(agent, rng)
        else
            metrics[:, i] = _update_agent_networks_base!(agent, rng)
        end
        _polyak_update!(agent)
    end
    return vec(mean(metrics; dims=2))
end

function _polyak_update!(agent::SACAgent)
    for (target, source) in zip(
        params(agent.networks.Q_1_target_layers),
        params(agent.networks.Q_1_layers),
    )
        @. target = (1 - agent.params.rho) * target + agent.params.rho * source
    end
    for (target, source) in zip(
        params(agent.networks.Q_2_target_layers),
        params(agent.networks.Q_2_layers),
    )
        @. target = (1 - agent.params.rho) * target + agent.params.rho * source
    end
    return nothing
end

function _update_agent_networks_tqc!(
    agent::SACAgent, rng::AbstractRNG = default_rng()
)
    losses = zeros(Float32, 7)

    ð¬, ðš, ð«áµ¥, ðáµ¥, ð¬â€² = sample_buffer(
        agent.memory, agent.params.minibatch_size, rng
    )
    ð = unsqueeze(ðáµ¥; dims=1)
    ð« = unsqueeze(ð«áµ¥; dims=1)

    Î¼â€², log_varâ€²áµ¤ = agent.networks.policy_layers(ð¬â€²)
    log_varâ€² = @. (
        agent.params.log_var_min
        + (tanh(log_varâ€²áµ¤) + 1)
        * (agent.params.log_var_max - agent.params.log_var_min)
        / 2
    )
    uâ€² = @. Î¼â€² + exp(log_varâ€² / 2) * $randn(rng, $eltype(Î¼â€²), $size(Î¼â€²))
    vâ€² = tanh.(uâ€²)
    ðšâ€² = @. vâ€² * agent.networks.action_scale + agent.networks.action_bias
    logÏ€â‚â€² = sum(
        @. (
            -0.5f0 * (log_varâ€² + (uâ€² - Î¼â€²) ^ 2 / exp(log_varâ€²) + log(2f0Ï€))
            - log(agent.networks.action_scale * (1 - vâ€² ^ 2) + eps(Float32))
        );
        dims=1,
    )
    ð¬â€²ðšâ€² = vcat(ð¬â€², ðšâ€²)
    ð¬ðš = vcat(ð¬, ðš)

    Qâ‚áµ€ = agent.networks.Q_1_target_layers(ð¬â€²ðšâ€²)
    Qâ‚‚áµ€ = agent.networks.Q_2_target_layers(ð¬â€²ðšâ€²)
    Qáµ€ = sort(vcat(Qâ‚áµ€, Qâ‚‚áµ€); dims=1)[1 : agent.params.k_q, :]
    y = unsqueeze(
        @. (
            ð«
            + agent.params.gamma
            * (1 - ð)
            * (Qáµ€ - exp(agent.networks.logÎ±) * logÏ€â‚â€²)
        );
        dims=2,
    )
    cumulative = unsqueeze(
        unsqueeze(
            @. ($collect(1:agent.params.n_q) - 0.5f0) / agent.params.n_q; dims=2
        );
        dims=1,
    )
    losses[4], âˆ‡ = withgradient(agent.networks.Q_1_layers) do m
        Qâ‚ = unsqueeze(m(ð¬ðš); dims=1)

        Î´ = y .- Qâ‚
        Î´â‚Š = abs.(Î´)

        huber_term = @. ifelse(Î´â‚Š > 1, Î´â‚Š - 0.5f0, 0.5f0 * Î´ ^ 2)
        loss = @. abs(cumulative - (Î´ < 0)) * huber_term
        return mean(loss)
    end
    update!(agent.opt_states[2], agent.networks.Q_1_layers, âˆ‡[1])
    losses[5], âˆ‡ = withgradient(agent.networks.Q_2_layers) do m
        Qâ‚‚ = unsqueeze(m(ð¬ðš); dims=1)

        Î´ = y .- Qâ‚‚
        Î´â‚Š = abs.(Î´)

        huber_term = @. ifelse(Î´â‚Š > 1, Î´â‚Š - 0.5f0, 0.5f0 * Î´ ^ 2)
        loss = @. abs(cumulative - (Î´ < 0)) * huber_term
        return mean(loss)
    end
    update!(agent.opt_states[3], agent.networks.Q_2_layers, âˆ‡[1])

    losses[1], âˆ‡ = withgradient(agent.networks.policy_layers) do m
        Î¼, log_var_u = m(ð¬)
        log_var = @. (
            agent.params.log_var_min
            + (tanh(log_var_u) + 1)
            * (agent.params.log_var_max - agent.params.log_var_min)
            / 2
        )
        u = @. Î¼ + exp(log_var / 2) * $randn(rng, $eltype(Î¼), $size(Î¼))
        v = tanh.(u)
        ðšÌƒ = @. v * agent.networks.action_scale + agent.networks.action_bias
        logÏ€á¶¿â‚Ìƒ = vec(
            sum(
                @. (
                    -0.5f0 * (log_var + (u - Î¼) ^ 2 / exp(log_var) + log(2f0Ï€))
                    - log(
                        agent.networks.action_scale * (1 - v ^ 2) + eps(Float32)
                    )
                );
                dims=1,
            )
        )
        ð¬ðšÌƒ = vcat(ð¬, ðšÌƒ)

        QÌƒâ‚ = unsqueeze(agent.networks.Q_1_layers(ð¬ðšÌƒ); dims=1)
        QÌƒâ‚‚ = unsqueeze(agent.networks.Q_2_layers(ð¬ðšÌƒ); dims=1)
        QÌƒ = vcat(QÌƒâ‚, QÌƒâ‚‚)
        QÌƒâ‚˜â‚‘â‚â‚™ = vec(mean(QÌƒ; dims=(1, 2)))

        ignore_derivatives() do
            losses[2] = -mean(logÏ€á¶¿â‚Ìƒ)
            losses[6] = mean(QÌƒâ‚)
            losses[7] = mean(QÌƒâ‚‚)
        end
        return mean(@. exp(agent.networks.logÎ±) * logÏ€á¶¿â‚Ìƒ - QÌƒâ‚˜â‚‘â‚â‚™)
    end
    update!(agent.opt_states[1], agent.networks.policy_layers, âˆ‡[1])

    losses[3], âˆ‡ = withgradient(agent.networks.logÎ±) do m
        return mean(@. m * (losses[2] - agent.params.H_bar))
    end
    update!(agent.opt_states[4], agent.networks.logÎ±, âˆ‡[1])
    return losses
end

function _update_agent_networks_base!(
    agent::SACAgent, rng::AbstractRNG = default_rng()
)
    losses = zeros(Float32, 7)

    ð¬, ðš, ð«, ð, ð¬â€² = sample_buffer(
        agent.memory, rng, agent.params.minibatch_size
    )

    Î¼â€², log_varâ€²áµ¤ = agent.networks.policy_layers(ð¬â€²)
    log_varâ€² = @. (
        agent.params.log_var_min
        + (tanh(log_varâ€²áµ¤) + 1)
        * (agent.params.log_var_max - agent.params.log_var_min)
        / 2
    )
    uâ€² = @. Î¼â€² + exp(log_varâ€² / 2) * $randn(rng, $eltype(Î¼â€²), $size(Î¼â€²))
    vâ€² = tanh.(uâ€²)
    ðšâ€² = @. vâ€² * agent.networks.action_scale + agent.networks.action_bias
    logÏ€â‚â€² = vec(
        sum(
            @. (
                -0.5f0 * (log_varâ€² + (uâ€² - Î¼â€²) ^ 2 / exp(log_varâ€²) + log(2f0Ï€))
                - log(agent.networks.action_scale * (1 - vâ€² ^ 2) + eps(Float32))
            );
            dims=1,
        )
    )
    ð¬â€²ðšâ€² = [ð¬â€²; ðšâ€²]
    ð¬ðš = [ð¬; ðš]

    Qâ‚áµ€ = vec(agent.networks.Q_1_target_layers(ð¬â€²ðšâ€²))
    Qâ‚‚áµ€ = vec(agent.networks.Q_2_target_layers(ð¬â€²ðšâ€²))
    Qáµ€ = min.(Qâ‚áµ€, Qâ‚‚áµ€)
    y = @. (
        ð«
        + agent.params.gamma
        * (1 - ð)
        * (Qáµ€ - exp(agent.networks.logÎ±) * logÏ€â‚â€²)
    )
    losses[4], âˆ‡ = withgradient(agent.networks.Q_1_layers) do m
        Qâ‚ = vec(m(ð¬ðš))
        return mse(Qâ‚, y)
    end
    update!(agent.opt_states[2], agent.networks.Q_1_layers, âˆ‡[1])
    losses[5], âˆ‡ = withgradient(agent.networks.Q_2_layers) do m
        Qâ‚‚ = vec(m(ð¬ðš))
        return mse(Qâ‚‚, y)
    end
    update!(agent.opt_states[3], agent.networks.Q_2_layers, âˆ‡[1])

    losses[1], âˆ‡ = withgradient(agent.networks.policy_layers) do m
        Î¼, log_var_u = m(ð¬)
        log_var = @. (
            agent.params.log_var_min
            + (tanh(log_var_u) + 1)
            * (agent.params.log_var_max - agent.params.log_var_min)
            / 2
        )
        u = @. Î¼ + exp(log_var / 2) * $randn(rng, $eltype(Î¼), $size(Î¼))
        v = tanh.(u)
        ðšÌƒ = @. v * agent.networks.action_scale + agent.networks.action_bias
        logÏ€á¶¿â‚Ìƒ = vec(
            sum(
                @. (
                    -0.5f0 * (log_var + (u - Î¼) ^ 2 / exp(log_var) + log(2f0Ï€))
                    - log(
                        agent.networks.action_scale * (1 - v ^ 2) + eps(Float32)
                    )
                );
                dims=1,
            )
        )
        ð¬ðšÌƒ = [ð¬; ðšÌƒ]
        Qâ‚ = vec(agent.networks.Q_1_layers(ð¬ðšÌƒ))
        Qâ‚‚ = vec(agent.networks.Q_2_layers(ð¬ðšÌƒ))
        Qâ‚˜áµ¢â‚™ = min.(Qâ‚, Qâ‚‚)
        ignore_derivatives() do
            losses[2] = -mean(logÏ€á¶¿â‚Ìƒ)
            losses[6] = mean(Qâ‚)
            losses[7] = mean(Qâ‚‚)
        end
        return mean(@. exp(agent.networks.logÎ±) * logÏ€á¶¿â‚Ìƒ - Qâ‚˜áµ¢â‚™)
    end
    update!(agent.opt_states[1], agent.networks.policy_layers, âˆ‡[1])
    losses[3], âˆ‡ = withgradient(agent.networks.logÎ±) do m
        return mean(@. m * (losses[2] - agent.params.H_bar))
    end
    update!(agent.opt_states[4], agent.networks.logÎ±, âˆ‡[1])
    return losses
end

function learn!(
    agent::SACAgent,
    env::QuantumControlEnvironment,
    init::Bool = true,
    rng::AbstractRNG = default_rng(),
)
    rewards = zeros(agent.params.episodes)
    losses = zeros(Float32, 7, agent.params.episodes)

    init && _initial_steps!(agent, env, rng)

    for episode in 1:agent.params.episodes
        r_episode = evaluation_steps!(agent, env, rng)
        l_episode = trainer_steps!(agent, rng)

        rewards[episode] = r_episode
        losses[:, episode] = l_episode
        println("Episode: ", episode, "| Rewards: ", rewards[episode])
    end
    return rewards, losses
end
