# """Soft actor-critic with options for using dropout Q-networks and truncated
# quantile critics.
# """

# """Parameters for the SAC algorithm with options of using DroQ or TQC.

# Fields:
#   * action_scale: The scale to match the action space of the continuous
#         environment.
#   * action_bias: The bias to match the action space of the continous
#         environment.
#   * HÃÑ: Target entropy, usually equals to -dimùíú.
#   * capacity: Number of transitions stored in memory (default: 100000).
#   * hiddens: Dimensions of hidden layers (default: [256, 256]).
#   * logœÉ¬≤‚Çò·µ¢‚Çô: Minium log standard deviation for stable training (default: -10).
#   * logœÉ¬≤‚Çò‚Çê‚Çì: Maximum log standard deviation for stable training (default: 3).
#   * use_tqc: Whether using a distribution of Q-networks (default: true).
#   * q‚Çô: Number of output q-values for TQC (default: 25).
#   * q‚Çñ: Bottom number of quintiles to keep for TQC (default: 21).
#   * dropout: Dropout probability for dropout Q-networks (default: 0.01).
#   * layer_norm: Whether to use layer normalisation after each layer for
#         Q-networks (default: true).
#   * Œ≥: Discount parameter for extrinsic/intrinsic rewards (default: 0.99).
#   * minibatch_size: Minibatch size for policy and Q-networks (default: 256).
#   * training_steps: Number of training steps per episode (default: 20).
#   * clips: Global gradient clippings (default: [5.0, 5.0, 5.0, 5.0]).
#   * decays: Weight decays (default: [0.0, 0.0, 0.0, 0.0]).
#   * Œ∑: Learning rates (default: [3e-4, 3e-4, 3e-4, 3e-4]).
#   * œÅ: Polyak update parameter (default: 0.005).
#   * warmup_normalisation_episodes: Number of initial episodes for observation
#         normalisation (default: 50).
#   * warmup_evaluation_episodes: Number of initial episodes to populate the
#         buffer (default: 50).
#   * episodes: Number of total episodes (default: 5000).
# """
# @kwdef struct SACParameters
#     action_scale::Vector{Float32}
#     action_bias::Vector{Float32}
#     HÃÑ::Float32
#     capacity::Int = 100000
#     hiddens::Vector{Int} = [256, 256]
#     logœÉ¬≤‚Çò·µ¢‚Çô::Float32 = -10
#     logœÉ¬≤‚Çò‚Çê‚Çì::Float32 = 3
#     use_tqc::Bool = true
#     q‚Çô::Int = 25
#     q‚Çñ::Int = 46
#     dropout::Float32 = 0.01
#     layer_norm::Bool = true
#     Œ≥::Float32 = 0.99
#     minibatch_size::Int = 256
#     training_steps::Int = 20
#     decays::Vector{Float32} = [0.0, 0.0, 0.0, 0.0]
#     clips::Vector{Float32} = [5.0, 5.0, 5.0, 5.0]
#     Œ∑::Vector{Float32} = [3e-4, 3e-4, 3e-4, 3e-4]
#     œÅ::Float32 = 0.005
#     warmup_normalisation_episodes::Int = 50
#     warmup_evaluation_episodes::Int = 50
#     episodes::Int = 1000
# end


# """SAC Agent.

# Struct of a agent that uses the Soft Actor Critic algorithm (arXiv: 1801.01290)
# with optional dropout Q-networks (arXiv: 2101.05982) and TQC (arXiv: 2005.04269)
# to improve sample efficiency.

# Args:
#   * env: The environment that the agent learns.
#   * kwargs: Keyword arguments for agent parameters, activation and
#         initialisation functions, and rngs.

# Fields:
#   * params: Hyper parameters for the agent.
#   * rng: Agent RNG.
#   * memory: Replay buffer with a history of transitions.
#   * networks: Neural networks.
#   * device: Device for neural networks.
#   * opt_states: Neural networks opt_states.
# """
# struct SACAgent{
#     R <: AbstractRNG,
#     M <: PrioritizedReplayBuffer,
#     ùí© <: SACNetworks,
#     ùí™ <: AbstractVector,
# } <: Agent{R, M, ùí©, ùí™}
#     params::SACParameters
#     rng::R
#     memory::M
#     networks::ùí©
#     opt_states::ùí™
# end

# function SACAgent(
#     env::QuantumControlEnvironment;
#     activation::Function = relu,
#     init::Function = glorot_uniform,
#     rng::AbstractRNG = default_rng(),
#     kwargs...,
# )
#     d‚Çí = length(env.observation_space)
#     d‚Çê = length(env.action_space)
#     continuous = env.action_space isa Vector{ClosedInterval{Float64}}
#     if continuous
#         action_scale = (
#             (rightendpoint.(env.action_space) - leftendpoint.(env.action_space))
#             / 2
#         )
#         action_bias = (
#             (rightendpoint.(env.action_space) + leftendpoint.(env.action_space))
#             / 2
#         )
#         HÃÑ = -d‚Çê
#     else
#         action_scale = zeros(d‚Çê)
#         action_bias = zeros(d‚Çê)
#         HÃÑ = 0.98 * log(d‚Çê)
#     end

#     params = SACParameters(
#         ; action_scale=action_scale, action_bias=action_bias, HÃÑ=HÃÑ, kwargs...
#     )
#     memory = PrioritizedReplayBuffer(continuous, d‚Çí, d‚Çê, params.capacity)
#     networks = SACNetworks(
#         continuous,
#         d‚Çí,
#         d‚Çê,
#         params.use_tqc ? params.q‚Çô : 1,
#         params.hiddens,
#         params.dropout,
#         params.layer_norm;
#         activation=activation,
#         init=init,
#         rng=rng,
#     )
#     opt_states = [
#         setup(
#             OptimiserChain(
#                 ClipNorm(params.clips[i]),
#                 AdamW(params.Œ∑[i], (0.9f0, 0.999f0), params.decays[i]),
#             ),
#             network,
#         )
#         for (i, network) in [
#             (1, networks.policy_layers),
#             (2, networks.Q‚ÇÅ_layers),
#             (3, networks.Q‚ÇÇ_layers),
#             (4, networks.logŒ±),
#         ]
#     ]
#     return SACAgent(params, rng, memory, networks, opt_states)
# end

# ###################
# # Getting actions #
# ###################
# function get_action(
#     agent::SACAgent{R, PrioritizedReplayBuffer{Float32}},
#     observation::Vector{Float64},
# ) where {R}
#     Œº, logœÉ¬≤·µ§ = agent.networks.policy_layers(f32(observation))
#     logœÉ¬≤ = @. (
#         agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#         + (tanh(logœÉ¬≤·µ§) + 1)
#         * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#         / 2
#     )
#     ùí©‚ÇÄ‚ÇÅ = randn(agent.rng, Float32, length(Œº))
#     action = @. (
#        tanh(Œº + exp(logœÉ¬≤ / 2) * ùí©‚ÇÄ‚ÇÅ) * agent.params.action_scale
#        + agent.params.action_bias
#     )
#     return convert(Vector{Float64}, action)
# end

# function get_random_action(
#     agent::SACAgent{R, PrioritizedReplayBuffer{Float32}},
#     action_space::Vector{<:ClosedInterval{Float64}},
# ) where {R}
#     return rand.(agent.rng, action_space)
# end

# #############################
# # Shared evaluation methods #
# #############################
# function evaluation_steps!(agent::SACAgent, env::QuantumControlEnvironment)
#     ‚àë·µ£ = zero(Float64)

#     observation = reset!(env)
#     done = false
#     while !done
#         index = update_and_get_index!(agent.memory)
#         agent.memory.observations[:, index] = observation

#         action = get_action(agent, observation)
#         agent.memory.actions[:, index] = action

#         observation, done, reward = step!(env, action)
#         agent.memory.observations‚Ä≤[:, index] = observation
#         # agent.memory.rewards[index] = reward[end]
#         agent.memory.rewards[index] = reward[1]
#         agent.memory.dones[index] = done
#         ‚àë·µ£ += reward[1]
#     end
#     return ‚àë·µ£
# end

# function _initial_steps!(agent::SACAgent, env::QuantumControlEnvironment)
#     if (
#         (env.observation_function isa NormalisedObservation)
#         | (env.reward_function isa NormalisedReward)
#     )
#         for _ in 1:agent.params.warmup_normalisation_episodes
#             _ = reset!(env)
#             done = false
#             while !done
#                 _, done, _ = step!(
#                     env, get_random_action(agent, env.action_space)
#                 )
#             end
#         end
#     end
#     for _ in 1:agent.params.warmup_evaluation_episodes
#         _ = evaluation_steps!(agent, env)
#     end
#     return nothing
# end

# ##########################
# # Shared trainer methods #
# ##########################
# function trainer_steps!(agent::SACAgent)
#     metrics = zeros(Float32, 7, agent.params.training_steps)
#     for i in 1:agent.params.training_steps
#         if agent.params.use_tqc
#             metrics[:, i] = _update_agent_networks_tqc!(agent)
#         else
#             metrics[:, i] = _update_agent_networks_base!(agent)
#         end
#         _polyak_update!(agent)
#     end
#     return vec(mean(metrics; dims=2))
# end

# function _polyak_update!(agent::SACAgent)
#     for (target, source) in zip(
#         params(agent.networks.Q‚ÇÅ_target_layers),
#         params(agent.networks.Q‚ÇÅ_layers),
#     )
#         @. target = (1 - agent.params.œÅ) * target + agent.params.œÅ * source
#     end
#     for (target, source) in zip(
#         params(agent.networks.Q‚ÇÇ_target_layers),
#         params(agent.networks.Q‚ÇÇ_layers),
#     )
#         @. target = (1 - agent.params.œÅ) * target + agent.params.œÅ * source
#     end
#     return nothing
# end

# #########################
# # Unique update methods #
# #########################
# function _update_agent_networks_tqc!(
#     agent::SACAgent{R, PrioritizedReplayBuffer{Float32}}
# ) where {R}
#     losses = zeros(Float32, 7)

#     ùê¨, ùêö, ùê´·µ•, ùêù·µ•, ùê¨‚Ä≤ = sample_buffer(
#         agent.memory, agent.rng, agent.params.minibatch_size
#     )
#     ùêù = unsqueeze(ùêù·µ•; dims=1)
#     ùê´ = unsqueeze(ùê´·µ•; dims=1)

#     Œº‚Ä≤, logœÉ¬≤‚Ä≤·µ§ = agent.networks.policy_layers(ùê¨‚Ä≤)
#     logœÉ¬≤‚Ä≤ = @. (
#         agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#         + (tanh(logœÉ¬≤‚Ä≤·µ§) + 1)
#         * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#         / 2
#     )
#     u‚Ä≤ = @. Œº‚Ä≤ + exp(logœÉ¬≤‚Ä≤ / 2) * $randn(agent.rng, $eltype(Œº‚Ä≤), $size(Œº‚Ä≤))
#     v‚Ä≤ = tanh.(u‚Ä≤)
#     ùêö‚Ä≤ = @. v‚Ä≤ * agent.params.action_scale + agent.params.action_bias
#     logœÄ‚Çê‚Ä≤ = sum(
#         @. (
#             -0.5f0 * (logœÉ¬≤‚Ä≤ + (u‚Ä≤ - Œº‚Ä≤) ^ 2 / exp(logœÉ¬≤‚Ä≤) + log(2f0œÄ))
#             - log(agent.params.action_scale * (1 - v‚Ä≤ ^ 2) + eps(Float32))
#         );
#         dims=1,
#     )
#     ùê¨‚Ä≤ùêö‚Ä≤ = vcat(ùê¨‚Ä≤, ùêö‚Ä≤)
#     ùê¨ùêö = vcat(ùê¨, ùêö)

#     Q‚ÇÅ·µÄ = agent.networks.Q‚ÇÅ_target_layers(ùê¨‚Ä≤ùêö‚Ä≤)
#     Q‚ÇÇ·µÄ = agent.networks.Q‚ÇÇ_target_layers(ùê¨‚Ä≤ùêö‚Ä≤)
#     Q·µÄ = sort(vcat(Q‚ÇÅ·µÄ, Q‚ÇÇ·µÄ); dims=1)[1 : agent.params.q‚Çñ, :]
#     y = unsqueeze(
#         @. (
#             ùê´
#             + agent.params.Œ≥
#             * (1 - ùêù)
#             * (Q·µÄ - exp(agent.networks.logŒ±) * logœÄ‚Çê‚Ä≤)
#         );
#         dims=2,
#     )
#     cumulative = unsqueeze(
#         unsqueeze(
#             @. ($collect(1:agent.params.q‚Çô) - 0.5f0) / agent.params.q‚Çô; dims=2
#         );
#         dims=1,
#     )
#     losses[4], ‚àá = withgradient(agent.networks.Q‚ÇÅ_layers) do m
#         Q‚ÇÅ = unsqueeze(m(ùê¨ùêö); dims=1)

#         Œ¥ = y .- Q‚ÇÅ
#         Œ¥‚Çä = abs.(Œ¥)

#         huber_term = @. ifelse(Œ¥‚Çä > 1, Œ¥‚Çä - 0.5f0, 0.5f0 * Œ¥ ^ 2)
#         loss = @. abs(cumulative - (Œ¥ < 0)) * huber_term
#         return mean(loss)
#     end
#     update!(agent.opt_states[2], agent.networks.Q‚ÇÅ_layers, ‚àá[1])
#     losses[5], ‚àá = withgradient(agent.networks.Q‚ÇÇ_layers) do m
#         Q‚ÇÇ = unsqueeze(m(ùê¨ùêö); dims=1)

#         Œ¥ = y .- Q‚ÇÇ
#         Œ¥‚Çä = abs.(Œ¥)

#         huber_term = @. ifelse(Œ¥‚Çä > 1, Œ¥‚Çä - 0.5f0, 0.5f0 * Œ¥ ^ 2)
#         loss = @. abs(cumulative - (Œ¥ < 0)) * huber_term
#         return mean(loss)
#     end
#     update!(agent.opt_states[3], agent.networks.Q‚ÇÇ_layers, ‚àá[1])

#     losses[1], ‚àá = withgradient(agent.networks.policy_layers) do m
#         Œº, logœÉ¬≤·µ§ = m(ùê¨)
#         logœÉ¬≤ = @. (
#             agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#             + (tanh(logœÉ¬≤·µ§) + 1)
#             * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#             / 2
#         )
#         u = @. Œº + exp(logœÉ¬≤ / 2) * $randn(agent.rng, $eltype(Œº), $size(Œº))
#         v = tanh.(u)
#         ùêöÃÉ = @. v * agent.params.action_scale + agent.params.action_bias
#         logœÄ·∂ø‚ÇêÃÉ = vec(
#             sum(
#                 @. (
#                     -0.5f0 * (logœÉ¬≤ + (u - Œº) ^ 2 / exp(logœÉ¬≤) + log(2f0œÄ))
#                     - log(
#                         agent.params.action_scale * (1 - v ^ 2) + eps(Float32)
#                     )
#                 );
#                 dims=1,
#             )
#         )
#         ùê¨ùêöÃÉ = vcat(ùê¨, ùêöÃÉ)

#         QÃÉ‚ÇÅ = unsqueeze(agent.networks.Q‚ÇÅ_layers(ùê¨ùêöÃÉ); dims=1)
#         QÃÉ‚ÇÇ = unsqueeze(agent.networks.Q‚ÇÇ_layers(ùê¨ùêöÃÉ); dims=1)
#         QÃÉ = vcat(QÃÉ‚ÇÅ, QÃÉ‚ÇÇ)
#         QÃÉ‚Çò‚Çë‚Çê‚Çô = vec(mean(QÃÉ; dims=(1, 2)))

#         ignore() do
#             losses[2] = -mean(logœÄ·∂ø‚ÇêÃÉ)
#             losses[6] = mean(QÃÉ‚ÇÅ)
#             losses[7] = mean(QÃÉ‚ÇÇ)
#         end
#         return mean(@. exp(agent.networks.logŒ±) * logœÄ·∂ø‚ÇêÃÉ - QÃÉ‚Çò‚Çë‚Çê‚Çô)
#     end
#     update!(agent.opt_states[1], agent.networks.policy_layers, ‚àá[1])

#     losses[3], ‚àá = withgradient(agent.networks.logŒ±) do m
#         return mean(@. m * (losses[2] - agent.params.HÃÑ))
#     end
#     update!(agent.opt_states[4], agent.networks.logŒ±, ‚àá[1])
#     return losses
# end

# function _update_agent_networks_base!(
#     agent::SACAgent{R, PrioritizedReplayBuffer{Float32}}
# ) where {R}
#     losses = zeros(Float32, 7)

#     ùê¨, ùêö, ùê´, ùêù, ùê¨‚Ä≤ = sample_buffer(
#         agent.memory, agent.rng, agent.params.minibatch_size
#     )

#     Œº‚Ä≤, logœÉ¬≤‚Ä≤·µ§ = agent.networks.policy_layers(ùê¨‚Ä≤)
#     logœÉ¬≤‚Ä≤ = @. (
#         agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#         + (tanh(logœÉ¬≤‚Ä≤·µ§) + 1)
#         * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#         / 2
#     )
#     u‚Ä≤ = @. Œº‚Ä≤ + exp(logœÉ¬≤‚Ä≤ / 2) * $randn(agent.rng, $eltype(Œº‚Ä≤), $size(Œº‚Ä≤))
#     v‚Ä≤ = tanh.(u‚Ä≤)
#     ùêö‚Ä≤ = @. v‚Ä≤ * agent.params.action_scale + agent.params.action_bias
#     logœÄ‚Çê‚Ä≤ = vec(
#         sum(
#             @. (
#                 -0.5f0 * (logœÉ¬≤‚Ä≤ + (u‚Ä≤ - Œº‚Ä≤) ^ 2 / exp(logœÉ¬≤‚Ä≤) + log(2f0œÄ))
#                 - log(agent.params.action_scale * (1 - v‚Ä≤ ^ 2) + eps(Float32))
#             );
#             dims=1,
#         )
#     )
#     ùê¨‚Ä≤ùêö‚Ä≤ = [ùê¨‚Ä≤; ùêö‚Ä≤]
#     ùê¨ùêö = [ùê¨; ùêö]

#     Q‚ÇÅ·µÄ = vec(agent.networks.Q‚ÇÅ_target_layers(ùê¨‚Ä≤ùêö‚Ä≤))
#     Q‚ÇÇ·µÄ = vec(agent.networks.Q‚ÇÇ_target_layers(ùê¨‚Ä≤ùêö‚Ä≤))
#     Q·µÄ = min.(Q‚ÇÅ·µÄ, Q‚ÇÇ·µÄ)
#     y = @. (
#         ùê´ + agent.params.Œ≥ * (1 - ùêù) * (Q·µÄ - exp(agent.networks.logŒ±) * logœÄ‚Çê‚Ä≤)
#     )
#     losses[4], ‚àá = withgradient(agent.networks.Q‚ÇÅ_layers) do m
#         Q‚ÇÅ = vec(m(ùê¨ùêö))
#         return mse(Q‚ÇÅ, y)
#     end
#     update!(agent.opt_states[2], agent.networks.Q‚ÇÅ_layers, ‚àá[1])
#     losses[5], ‚àá = withgradient(agent.networks.Q‚ÇÇ_layers) do m
#         Q‚ÇÇ = vec(m(ùê¨ùêö))
#         return mse(Q‚ÇÇ, y)
#     end
#     update!(agent.opt_states[3], agent.networks.Q‚ÇÇ_layers, ‚àá[1])

#     losses[1], ‚àá = withgradient(agent.networks.policy_layers) do m
#         Œº, logœÉ¬≤·µ§ = m(ùê¨)
#         logœÉ¬≤ = @. (
#             agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#             + (tanh(logœÉ¬≤·µ§) + 1)
#             * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#             / 2
#         )
#         u = @. Œº + exp(logœÉ¬≤ / 2) * $randn(agent.rng, $eltype(Œº), $size(Œº))
#         v = tanh.(u)
#         ùêöÃÉ = @. v * agent.params.action_scale + agent.params.action_bias
#         logœÄ·∂ø‚ÇêÃÉ = vec(
#             sum(
#                 @. (
#                     -0.5f0 * (logœÉ¬≤ + (u - Œº) ^ 2 / exp(logœÉ¬≤) + log(2f0œÄ))
#                     - log(
#                         agent.params.action_scale * (1 - v ^ 2) + eps(Float32)
#                     )
#                 );
#                 dims=1,
#             )
#         )
#         ùê¨ùêöÃÉ = [ùê¨; ùêöÃÉ]
#         Q‚ÇÅ = vec(agent.networks.Q‚ÇÅ_layers(ùê¨ùêöÃÉ))
#         Q‚ÇÇ = vec(agent.networks.Q‚ÇÇ_layers(ùê¨ùêöÃÉ))
#         Q‚Çò·µ¢‚Çô = min.(Q‚ÇÅ, Q‚ÇÇ)
#         ignore() do
#             losses[2] = -mean(logœÄ·∂ø‚ÇêÃÉ)
#             losses[6] = mean(Q‚ÇÅ)
#             losses[7] = mean(Q‚ÇÇ)
#         end
#         return mean(@. exp(agent.networks.logŒ±) * logœÄ·∂ø‚ÇêÃÉ - Q‚Çò·µ¢‚Çô)
#     end
#     update!(agent.opt_states[1], agent.networks.policy_layers, ‚àá[1])
#     losses[3], ‚àá = withgradient(agent.networks.logŒ±) do m
#         return mean(@. m * (losses[2] - agent.params.HÃÑ))
#     end
#     update!(agent.opt_states[4], agent.networks.logŒ±, ‚àá[1])
#     return losses
# end

# ##################
# # Agent learning #
# ##################
# function learn!(agent::SACAgent, env::QuantumControlEnvironment)
#     rewards = zeros(agent.params.episodes)
#     losses = zeros(Float32, 7, agent.params.episodes)

#     _initial_steps!(agent, env)

#     for episode in 1:agent.params.episodes
#         episode·µ£ = evaluation_steps!(agent, env)
#         episode‚Çó = trainer_steps!(agent)

#         rewards[episode] = episode·µ£
#         losses[:, episode] = episode‚Çó
#         println("Episode: ", episode, "| Rewards: ", rewards[episode])
#     end
#     return rewards, losses
# end
