# """Proximal policy optimisation with dual value networks."""

# """Parameters for PPO agent with DNA.

# Fields:
#   * action_scale: The scale to match the action space of the continuous
#         environment.
#   * action_bias: The bias to match the action space of the continous
#         environment.
#   * capacity: Number of transitions stored in memory (default: 512).
#   * Œ≥: Discount parameter (default: 0.999).
#   * Œª: Parameter for GAEs (default: [0.97, 0.97]).
#   * hiddens: Dimensions of hidden layers (default: [128, 128]).
#   * logœÉ¬≤‚Çò·µ¢‚Çô: Minium log standard deviation for stable training (default: -10).
#   * logœÉ¬≤‚Çò‚Çê‚Çì: Maximum log standard deviation for stable training (default: 3).
#   * minibatch_sizes : Minibatch size for policy, value and distillation phases
#         (default: [128, 64, 64]).
#   * training_steps: Number of training steps per epoch for the network
#         parameters (default: [2, 2, 2]).
#   * clips: Global gradient clippings for phases (default: [5.0, 5.0, 5.0]).
#   * decay: Weight decays for phases(default: [1e-4, 1e-4, 1e-4]).
#   * Œ∑: Learning rates for phases (default: [3e-4, 3e-4, 3e-4]).
#   * ùúÄ: PPO clip parameter (default: 0.2).
#   * Œ±: Entropy bonus weight (default: 0.0).
#   * Œ≤: KL divergence weight (default: 1.0).
#   * warmup_normalisation_episodes: Number of initial episodes for observation
#         normalisation (default: 50).
#   * epochs: Number of training epochs (default: 10000).
# """
# @kwdef struct PPOParameters <: AgentParameters
#     action_scale::Vector{Float32}
#     action_bias::Vector{Float32}
#     capacity::Int = 512
#     Œ≥::Float32 = 0.99
#     Œª::Vector{Float32} = [0.97, 0.97]
#     hiddens::Vector{Int} = [128, 128]
#     logœÉ¬≤‚Çò·µ¢‚Çô::Float32 = -10
#     logœÉ¬≤‚Çò‚Çê‚Çì::Float32 = 3
#     minibatch_sizes::Vector{Int} = [128, 64, 64]
#     training_steps::Vector{Int} = [2, 2, 2]
#     decays::Vector{Float32} = [0.0, 0.0, 0.0]
#     clips::Vector{Float32} = [5.0, 5.0, 5.0]
#     Œ∑::Vector{Float32} = [3e-4, 3e-4, 3e-4]
#     ùúÄ::Float32 = 0.2
#     Œ±::Float32 = 0.0
#     Œ≤::Float32 = 1.0
#     warmup_normalisation_episodes::Int = 50
#     epochs::Int = 1000
# end

# """PPO Agent.

# Struct of a PPO agent with options for PPO loss (arXiv: 1707.06347) with either
# the standard actor-critic shared network architecture or a dual network
# architecture (DNA) (arXiv: 2206.10027).

# Args:
#   * env: The environment that the agent learns.
#   * kwargs: Keyword arguments for agent parameters, activation and
#         initialisation functions, and rngs.

# Fields:
#   * params: Hyper parameters for the agent.
#   * rng: Agent RNG.
#   * memory: Replay buffer with a history of transitions.
#   * logœÄ·µíÀ°·µà: Old agent policy / policy parameters.
#   * networks: Neural networks.
#   * opt_states: Neural networks optimiser states.
# """
# struct PPOAgent{
#     R <: AbstractRNG,
#     M <: CircularReplayBuffer,
#     # ùíü <: Union{FluxCPUDevice, FluxCUDADevice},
#     ùí© <: DualNetworksArchitecture,
#     ùí™ <: AbstractVector,
# } <: Agent{R, M, ùí©, ùí™}
#     params::PPOParameters
#     rng::R
#     memory::M
#     logœÄ·µíÀ°·µà::Matrix{Float32}
#     # device::ùíü
#     networks::ùí©
#     opt_states::ùí™
# end

# function PPOAgent(
#     env::QuantumControlEnvironment;
#     # device::Union{FluxCPUDevice, FluxCUDADevice} = get_device(),
#     activation::Function = relu,
#     init::Function = glorot_uniform,
#     rng::AbstractRNG = default_rng(),
#     kwargs...,
# )
#     d‚Çí = length(env.observation_space)
#     d‚Çê = length(env.action_space)
#     continuous = env.action_space isa Vector{ClosedInterval{eltype(env)}}
#     recurrence = d‚Çí == d‚Çê + 1
#     if continuous
#         action_scale = (
#             (rightendpoint.(env.action_space) - leftendpoint.(env.action_space))
#             / 2
#         )
#         action_bias = (
#             (rightendpoint.(env.action_space) + leftendpoint.(env.action_space))
#             / 2
#         )
#     else
#         action_scale = zeros(d‚Çê)
#         action_bias = zeros(d‚Çê)
#     end

#     params = PPOParameters(
#         ; action_scale=action_scale, action_bias=action_bias, kwargs...
#     )
#     if recurrence
#         if !iszero(mod(params.capacity, env.model_function.t‚Çô))
#             throw(
#                 ArgumentError(
#                     "If using recurrence, the capacity must be a multiple of"
#                     * " the number of time steps."
#                 )
#             )
#         end
#     end
#     networks = DualNetworksArchitecture(
#         continuous,
#         d‚Çí,
#         d‚Çê,
#         params.hiddens,
#         recurrence;
#         activation=activation,
#         init=init,
#         rng=rng,
#     )
#     opt_states = [
#         setup(
#             OptimiserChain(
#                 ClipNorm(params.clips[i]),
#                 AdamW(params.Œ∑[i], (0.9f0, 0.999f0), params.decays[i]),
#             ),
#             network,
#         )
#         for (i, network) in [
#             (1, networks.actor_critic),
#             (2, networks.value_layers),
#             (3, networks.actor_critic),
#         ]
#     ]
#     memory = CircularReplayBuffer(
#         continuous,
#         d‚Çí,
#         d‚Çê,
#         params.capacity,
#         recurrence,
#         params.hiddens[1],
#     )
#     logœÄ·µíÀ°·µà = ones(Float32, d‚Çê + continuous * d‚Çê, params.capacity)
#     return PPOAgent(params, rng, memory, logœÄ·µíÀ°·µà, networks, opt_states)
# end

# ###################
# # Getting actions #
# ###################
# function get_action(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Nothing}},
#     observation::Vector{<:AbstractFloat},
# ) where {R}
#     Œº, logœÉ¬≤·µ§ = get_œÄÃÉ(agent.networks, f32(observation))
#     action, untransformed_action = _get_and_transform_action(agent, Œº, logœÉ¬≤·µ§)
#     return convert(typeof(observation), action), untransformed_action
# end

# function get_action(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Matrix{Float32}}},
#     cell_state::Matrix{Float32},
#     observation::Vector{<:AbstractFloat},
# ) where {R}
#     cell_state, (Œº, logœÉ¬≤·µ§) = get_œÄÃÉ(
#         agent.networks, cell_state, f32(observation)
#     )
#     action, untransformed_action = _get_and_transform_action(agent, Œº, logœÉ¬≤·µ§)
#     return (
#         cell_state, convert(typeof(observation), action), untransformed_action
#     )
# end

# function get_random_action(
#     agent::PPOAgent,
#     action_space::Vector{<:ClosedInterval{<:AbstractFloat}},
# )
#     return rand.(agent.rng, action_space)
# end

# function _get_and_transform_action(
#     agent::PPOAgent, Œº::Vector{Float32}, logœÉ¬≤·µ§::Vector{Float32}
# )
#     logœÉ¬≤ = @. (
#         agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#         + (tanh(logœÉ¬≤·µ§) + 1)
#         * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#         / 2
#     )
#     ùí©‚ÇÄ‚ÇÅ = randn(agent.rng, eltype(Œº), length(Œº))
#     untransformed_action = @. Œº + exp(logœÉ¬≤ / 2) * ùí©‚ÇÄ‚ÇÅ
#     action = @. (
#         tanh(untransformed_action) * agent.params.action_scale
#         + agent.params.action_bias
#     )
#     return action, untransformed_action
# end

# ######################
# # Evaluation methods #
# ######################
# function evaluation_steps!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Nothing}},
#     env::QuantumControlEnvironment,
# ) where {R}
#     ‚àë·µ£ = zero(eltype(env))

#     observation = reset!(env)
#     for i in 1:agent.memory.capacity
#         agent.memory.observations[:, i] = observation

#         action, untransformed_action = get_action(agent, observation)
#         agent.memory.actions[:, i] = untransformed_action
#         # agent.memory.actions[:, i] = action

#         observation, done, reward = step!(env, action)
#         agent.memory.rewards[i] = reward[end]
#         # agent.memory.rewards[i] = reward[1]
#         agent.memory.dones[i] = done
#         ‚àë·µ£ += reward[1]
#         if done
#             observation = reset!(env)
#         end
#     end
#     agent.memory.observations[:, end] = observation
#     return ‚àë·µ£
# end

# function evaluation_steps!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Matrix{Float32}}},
#     env::QuantumControlEnvironment,
# ) where{R}
#     ‚àë·µ£ = zero(eltype(env))

#     observation = reset!(env)
#     cell_state = zeros(Float32, agent.params.hiddens[1], 1)
#     for i in 1:agent.memory.capacity
#         agent.memory.observations[:, i] = observation
#         agent.memory._cell_states[:, i] = cell_state

#         cell_state, action, untransformed_action = get_action(
#             agent, cell_state, observation
#         )
#         agent.memory.actions[:, i] = untransformed_action
#         # agent.memory.actions[:, i] = action

#         observation, done, reward = step!(env, action)
#         agent.memory.rewards[i] = reward[end]
#         # agent.memory.rewards[i] = reward[1]
#         agent.memory.dones[i] = done
#         ‚àë·µ£ += reward[1]
#         if done
#             observation = reset!(env)
#             cell_state = zeros(Float32, agent.params.hiddens[1], 1)
#         end
#     end
#     agent.memory.observations[:, end] = observation
#     agent.memory._cell_states[:, end] = cell_state
#     return ‚àë·µ£
# end

# function _initial_steps(agent::PPOAgent, env::QuantumControlEnvironment)
#     for _ in 1:agent.params.warmup_normalisation_episodes
#         _ = reset!(env)
#         done = false
#         while !done
#             _, done, _ = step!(env, get_random_action(agent, env.action_space))
#         end
#     end
#     return nothing
# end

# ###################
# # Trainer methods #
# ###################
# function trainer_steps!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Nothing}}
# ) where {R}
#     œÄÃÉ, _, V·µ• = agent.networks(agent.memory.observations)
#     bootstrap_reward!(agent.memory, V·µ•[end], agent.params.Œ≥)
#     _update_logœÄ·µíÀ°·µà!(agent, œÄÃÉ)

#     advantages = calculate_advantages(
#         agent.memory, vec(V·µ•), agent.params.Œ≥, agent.params.Œª[1]
#     )
#     targets = calculate_targets(
#         agent.memory, vec(V·µ•), agent.params.Œ≥, agent.params.Œª[2]
#     )
#     # targets = calculate_returns(agent.memory, agent.params.Œ≥)

#     losses = zeros(Float32, 6)
#     losses[1:3] = _update_agent_policy!(agent, advantages)
#     losses[4] = _update_agent_value!(agent, targets)
#     losses[5:6] = _update_agent_distillation!(agent)
#     return losses
# end

# function trainer_steps!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Matrix{Float32}}}
# ) where {R}
#     œÄÃÉ, _, V·µ• = agent.networks(
#         agent.memory._cell_states, agent.memory.observations
#     )
#     bootstrap_reward!(agent.memory, V·µ•[end], agent.params.Œ≥)
#     _update_logœÄ·µíÀ°·µà!(agent, œÄÃÉ)

#     advantages = calculate_advantages(
#         agent.memory, vec(V·µ•), agent.params.Œ≥, agent.params.Œª[1]
#     )
#     targets = calculate_targets(
#         agent.memory, vec(V·µ•), agent.params.Œ≥, agent.params.Œª[2]
#     )
#     # targets = calculate_returns(agent.memory, agent.params.Œ≥)

#     losses = zeros(Float32, 6)
#     losses[1:3] = _update_agent_policy!(agent, advantages)
#     losses[4] = _update_agent_value!(agent, targets)
#     losses[5:6] = _update_agent_distillation!(agent)
#     return losses
# end

# function _update_logœÄ·µíÀ°·µà!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, C}},
#     œÄÃÉ::Tuple{Matrix{Float32}, Matrix{Float32}},
# ) where {R, C}
#     half = length(agent.params.action_scale)
#     agent.logœÄ·µíÀ°·µà[1:half, :] = œÄÃÉ[1][:, 1 : end - 1]
#     agent.logœÄ·µíÀ°·µà[half + 1 : end, :] = œÄÃÉ[2][:, 1 : end - 1]
#     return nothing
# end

# ###########################################
# # Continuous non-recurrent update methods #
# ###########################################
# function _update_agent_policy!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Nothing}},
#     advantages::Vector{Float32},
# ) where {R}
#     losses = zeros(Float32, 3, agent.params.training_steps[1])

#     half = length(agent.params.action_scale)
#     for i in 1:agent.params.training_steps[1]
#         for (batch·µí, batch·µíÀ°·µà, batch·µÉ, batch·¥¨) in DataLoader(
#             (
#                 agent.memory.observations[:, 1 : end - 1],
#                 agent.logœÄ·µíÀ°·µà,
#                 agent.memory.actions,
#                 advantages,
#             );
#             batchsize=agent.params.minibatch_sizes[1],
#         )
#             Œº·µíÀ°·µà = batch·µíÀ°·µà[1:half, :]
#             logœÉ¬≤·µíÀ°·µà = @. (
#                 agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                 + (tanh(batch·µíÀ°·µà[half + 1 : end, :]) + 1)
#                 * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                 / 2
#             )
#             logœÄ·µíÀ°·µà‚Çê = vec(
#                 sum(
#                     @. -(logœÉ¬≤·µíÀ°·µà + (batch·µÉ - Œº·µíÀ°·µà) ^ 2 / exp(logœÉ¬≤·µíÀ°·µà)) / 2;
#                     dims=1,
#                 )
#             )
#             ‚àá = gradient(agent.networks.actor_critic) do m
#                 Œº, logœÉ¬≤·µ§ = get_œÄÃÉ(m, batch·µí)
#                 logœÉ¬≤ = @. (
#                     agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                     + (tanh(logœÉ¬≤·µ§) + 1)
#                     * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                     / 2
#                 )
#                 logœÄ·∂ø‚Çê = vec(
#                     sum(@. -(logœÉ¬≤ + (batch·µÉ - Œº) ^ 2 / exp(logœÉ¬≤)) / 2; dims=1)
#                 )
#                 ratio = @. exp(logœÄ·∂ø‚Çê - logœÄ·µíÀ°·µà‚Çê)
#                 L = batch·¥¨ .* ratio
#                 g = @. (
#                     batch·¥¨
#                     * clamp(ratio, 1 - agent.params.ùúÄ, 1 + agent.params.ùúÄ)
#                 )
#                 policy_loss = -mean(min.(L, g))
#                 entropy = mean(vec(sum(logœÉ¬≤ ./ 2; dims=1)))
#                 entropy_bonus = -agent.params.Œ± * entropy
#                 ignore() do
#                     losses[1, i] += policy_loss
#                     losses[2, i] += entropy
#                     losses[3, i] += entropy_bonus
#                 end
#                 return policy_loss + entropy_bonus
#             end
#             update!(agent.opt_states[1], agent.networks.actor_critic, ‚àá[1])
#         end
#         losses[:, i] ./= agent.params.capacity √∑ agent.params.minibatch_sizes[1]
#     end
#     return vec(mean(losses; dims=2))
# end

# function _update_agent_value!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Nothing}},
#     targets::Vector{Float32},
# ) where {R}
#     loss = zero(Float32)

#     for _ in 1:agent.params.training_steps[2]
#         for (batch·µí, batch·µÄ) in DataLoader(
#             (agent.memory.observations[:, 1 : end - 1], targets);
#             batchsize=agent.params.minibatch_sizes[2],
#         )
#             l, ‚àá = withgradient(agent.networks.value_layers) do m
#                 V·µ† = vec(m(batch·µí))
#                 return mse(V·µ†, batch·µÄ)
#             end
#             loss += l
#             update!(agent.opt_states[2], agent.networks.value_layers, ‚àá[1])
#         end
#         loss /= agent.params.capacity √∑ agent.params.minibatch_sizes[2]
#     end
#     return loss / agent.params.training_steps[2]
# end

# function _update_agent_distillation!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Nothing}}
# ) where {R}
#     losses = zeros(Float32, 2, agent.params.training_steps[3])

#     œÄÃÉ = get_œÄÃÉ(agent.networks.actor_critic, agent.memory.observations)
#     _update_logœÄ·µíÀ°·µà!(agent, œÄÃÉ)

#     half = length(agent.params.action_scale)
#     for i in 1:agent.params.training_steps[3]
#         for (batch·µí, Œº·µíÀ°·µà, logœÉ·µ§¬≤·µíÀ°·µà) in DataLoader(
#             (
#                 agent.memory.observations[:, 1 : end - 1],
#                 agent.logœÄ·µíÀ°·µà[1:half, :],
#                 agent.logœÄ·µíÀ°·µà[half + 1 : end, :],
#             );
#             batchsize=agent.params.minibatch_sizes[3],
#         )
#             V·µ† = agent.networks.value_layers(batch·µí)
#             logœÉ¬≤·µíÀ°·µà = @. (
#                 agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                 + (tanh(logœÉ·µ§¬≤·µíÀ°·µà) + 1)
#                 * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                 / 2
#             )
#             ‚àá = gradient(agent.networks.actor_critic) do m
#                 (Œº, logœÉ·µ§¬≤), V·∂ø = m(batch·µí)
#                 logœÉ¬≤ = @. (
#                     agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                     + (tanh(logœÉ·µ§¬≤) + 1)
#                     * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                     / 2
#                 )
#                 t‚ÇÅ = @. (Œº·µíÀ°·µà - Œº) ^ 2 / exp(logœÉ¬≤)
#                 t‚ÇÇ = @. exp(logœÉ¬≤·µíÀ°·µà - logœÉ¬≤)
#                 t‚ÇÉ = @. (logœÉ¬≤ - logœÉ¬≤·µíÀ°·µà)
#                 kl_divergence = agent.params.Œ≤ * mean(t‚ÇÅ + t‚ÇÇ + t‚ÇÉ) / 2
#                 # kl_divergence = agent.params.Œ≤ * mse(Œº, Œº·µíÀ°·µà)
#                 value_distillation = mse(V·∂ø, V·µ†)
#                 ignore() do
#                     losses[1, i] += value_distillation
#                     losses[2, i] += kl_divergence
#                 end
#                 return value_distillation + kl_divergence
#             end
#             update!(agent.opt_states[3], agent.networks.actor_critic, ‚àá[1])
#         end
#         losses[:, i] ./= agent.params.capacity √∑ agent.params.minibatch_sizes[3]
#     end
#     return vec(mean(losses; dims=2))
# end

# #######################################
# # Continuous recurrent update methods #
# #######################################
# function _update_agent_policy!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Matrix{Float32}}},
#     advantages::Vector{Float32},
# ) where {R}
#     losses = zeros(Float32, 3, agent.params.training_steps[1])

#     half = length(agent.params.action_scale)
#     for i in 1:agent.params.training_steps[1]
#         for (batch·µí, batch·∂ú, batch·µíÀ°·µà, batch·µÉ, batch·¥¨) in DataLoader(
#             (
#                 agent.memory.observations[:, 1 : end - 1],
#                 agent.memory._cell_states[:, 1 : end - 1],
#                 agent.logœÄ·µíÀ°·µà,
#                 agent.memory.actions,
#                 advantages,
#             );
#             batchsize=agent.params.minibatch_sizes[1],
#         )
#             Œº·µíÀ°·µà = batch·µíÀ°·µà[1:half, :]
#             logœÉ¬≤·µíÀ°·µà = @. (
#                 agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                 + (tanh(batch·µíÀ°·µà[half + 1 : end, :]) + 1)
#                 * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                 / 2
#             )
#             logœÄ·µíÀ°·µà‚Çê = vec(
#                 sum(
#                     @. -(logœÉ¬≤·µíÀ°·µà + (batch·µÉ - Œº·µíÀ°·µà) ^ 2 / exp(logœÉ¬≤·µíÀ°·µà)) / 2;
#                     dims=1,
#                 )
#             )
#             # ùê®, ùê° = (
#             #     reshape(batch·µí, size(batch·µí, 1), 38, 5),
#             #     reshape(batch·∂ú, size(batch·∂ú, 1), 38, 5),
#             # )
#             ùê® = reshape(batch·µí, size(batch·µí, 1), 38, 5)
#             ‚àá = gradient(agent.networks.actor_critic) do m
#                 _, (Œº, logœÉ¬≤·µ§) = get_œÄÃÉ(m, batch·∂ú, batch·µí)
#                 # _, (Œº, logœÉ¬≤·µ§) = get_œÄÃÉ(m, ùê°, ùê®)
#                 logœÉ¬≤ = @. (
#                     agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                     + (tanh(logœÉ¬≤·µ§) + 1)
#                     * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                     / 2
#                 )
#                 logœÄ·∂ø‚Çê = vec(
#                     sum(@. -(logœÉ¬≤ + (batch·µÉ - Œº) ^ 2 / exp(logœÉ¬≤)) / 2; dims=1)
#                 )
#                 ratio = @. exp(logœÄ·∂ø‚Çê - logœÄ·µíÀ°·µà‚Çê)
#                 L = batch·¥¨ .* ratio
#                 g = @. (
#                     batch·¥¨
#                     * clamp(ratio, 1 - agent.params.ùúÄ, 1 + agent.params.ùúÄ)
#                 )
#                 policy_loss = -mean(min.(L, g))
#                 entropy = mean(vec(sum(logœÉ¬≤ ./ 2; dims=1)))
#                 entropy_bonus = -agent.params.Œ± * entropy
#                 ignore() do
#                     losses[1, i] += policy_loss
#                     losses[2, i] += entropy
#                     losses[3, i] += entropy_bonus
#                 end
#                 return policy_loss + entropy_bonus
#             end
#             update!(agent.opt_states[1], agent.networks.actor_critic, ‚àá[1])
#         end
#         losses[:, i] ./= agent.params.capacity √∑ agent.params.minibatch_sizes[1]
#     end
#     return vec(mean(losses; dims=2))
# end

# function _update_agent_value!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Matrix{Float32}}},
#     targets::Vector{Float32},
# ) where {R}
#     loss = zero(Float32)

#     for _ in 1:agent.params.training_steps[2]
#         for (batch·µí, batch·∂ú, batch·µÄ) in DataLoader(
#             (
#                 agent.memory.observations[:, 1 : end - 1],
#                 agent.memory._cell_states[:, 1 : end - 1],
#                 targets,
#             );
#             batchsize=agent.params.minibatch_sizes[2],
#         )
#             l, ‚àá = withgradient(agent.networks.value_layers) do m
#                 V·µ† = vec(get_V·µ•(m, batch·∂ú, batch·µí))
#                 return mse(V·µ†, batch·µÄ)
#             end
#             loss += l
#             update!(agent.opt_states[2], agent.networks.value_layers, ‚àá[1])
#         end
#         loss /= agent.params.capacity √∑ agent.params.minibatch_sizes[2]
#     end
#     return loss / agent.params.training_steps[2]
# end

# function _update_agent_distillation!(
#     agent::PPOAgent{R, CircularReplayBuffer{Float32, Matrix{Float32}}}
# ) where {R}
#     losses = zeros(Float32, 2, agent.params.training_steps[3])

#     _, œÄÃÉ = get_œÄÃÉ(
#         agent.networks.actor_critic,
#         agent.memory._cell_states,
#         agent.memory.observations,
#     )
#     _update_logœÄ·µíÀ°·µà!(agent, œÄÃÉ)

#     half = length(agent.params.action_scale)
#     for i in 1:agent.params.training_steps[3]
#         for (batch·µí, batch·∂ú, Œº·µíÀ°·µà, logœÉ·µ§¬≤·µíÀ°·µà) in DataLoader(
#             (
#                 agent.memory.observations[:, 1 : end - 1],
#                 agent.memory._cell_states[:, 1 : end - 1],
#                 agent.logœÄ·µíÀ°·µà[1:half, :],
#                 agent.logœÄ·µíÀ°·µà[half + 1 : end, :],
#             );
#             batchsize=agent.params.minibatch_sizes[3],
#         )
#             V·µ† = get_V·µ•(agent.networks.value_layers, batch·∂ú, batch·µí)
#             logœÉ¬≤·µíÀ°·µà = @. (
#                 agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                 + (tanh(logœÉ·µ§¬≤·µíÀ°·µà) + 1)
#                 * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                 / 2
#             )
#             ‚àá = gradient(agent.networks.actor_critic) do m
#                 (Œº, logœÉ·µ§¬≤), V·∂ø = m(batch·∂ú, batch·µí)
#                 logœÉ¬≤ = @. (
#                     agent.params.logœÉ¬≤‚Çò·µ¢‚Çô
#                     + (tanh(logœÉ·µ§¬≤) + 1)
#                     * (agent.params.logœÉ¬≤‚Çò‚Çê‚Çì - agent.params.logœÉ¬≤‚Çò·µ¢‚Çô)
#                     / 2
#                 )
#                 t‚ÇÅ = @. (Œº·µíÀ°·µà - Œº) ^ 2 / exp(logœÉ¬≤)
#                 t‚ÇÇ = @. exp(logœÉ¬≤·µíÀ°·µà - logœÉ¬≤)
#                 t‚ÇÉ = @. (logœÉ¬≤ - logœÉ¬≤·µíÀ°·µà)
#                 kl_divergence = agent.params.Œ≤ * mean(t‚ÇÅ + t‚ÇÇ + t‚ÇÉ) / 2
#                 # kl_divergence = agent.params.Œ≤ * mse(Œº, Œº·µíÀ°·µà)
#                 value_distillation = mse(V·∂ø, V·µ†)
#                 ignore() do
#                     losses[1, i] += value_distillation
#                     losses[2, i] += kl_divergence
#                 end
#                 return value_distillation + kl_divergence
#             end
#             update!(agent.opt_states[3], agent.networks.actor_critic, ‚àá[1])
#         end
#         losses[:, i] ./= agent.params.capacity √∑ agent.params.minibatch_sizes[3]
#     end
#     return vec(mean(losses; dims=2))
# end

# #####################
# # Discrete  methods #
# #####################
# # function get_action(
# #     agent::PPOAgent{R, CircularReplayBuffer{Int, Nothing}},
# #     observation::Vector{<:AbstractFloat},
# #     time_step::Int,
# # ) where {R}
# #     logits = get_œÄÃÉ(agent.networks, f32(observation))
# #     action = wsample(agent.rng, softmax(logits))
# #     agent.memory.actions[:, time_step] = action
# #     return action
# # end

# # function get_action(
# #     agent::PPOAgent{R, CircularReplayBuffer{Int, Nothing}},
# #     cell_state::Matrix{Float32},
# #     observation::Vector{Float32},
# # ) where {R}
# #     logits = get_œÄÃÉ(agent.networks, observation)
# #     action = wsample(agent.rng, softmax(logits))
# #     return action
# # end

# # function get_random_action(
# #     agent::PPOAgent{R, CircularReplayBuffer{Int}}, action_space::OneTo{Int}
# # ) where {R}
# #     return rand(agent.rng, action_space)
# # end

# # function _update_logœÄ·µíÀ°·µà!(
# #     agent::PPOAgent{T, R, PT, CircularReplayBuffer{T, Int}}, œÄÃÉ::Matrix{T}
# # ) where {T, R, PT}
# #     agent.logœÄ·µíÀ°·µà .= logsoftmax(œÄÃÉ[:, 1 : end - 1]; dims=1)
# #     return nothing
# # end

# # function _update_agent_policy!(
# #     agent::PPOAgent{T, PT, CircularReplayBuffer{T, Int}}, advantages::Matrix{T}
# # ) where {T, PT}
# #     losses = zeros(T, 3, agent.params.training_steps[1])

# #     for i in 1:agent.params.training_steps[1]
# #         for (batch·µí, batch·µíÀ°·µà, batch·µÉ, batch·¥¨) in DataLoader(
# #             (
# #                 agent.memory.observations[:, 1 : end - 1],
# #                 agent.logœÄ·µíÀ°·µà,
# #                 agent.memory.actions,
# #                 advantages,
# #             );
# #             batchsize=agent.params.minibatch_sizes[1],
# #         )
# #             logœÄ·µíÀ°·µà‚Çê = [
# #                 batch·µíÀ°·µà[batch·µÉ[i], i]
# #                 for i in 1:agent.params.minibatch_sizes[1]
# #             ]
# #             ‚àá = gradient(agent.networks.actor_critic) do m
# #                 logits = get_œÄÃÉ(m, batch·µí)
# #                 œÄ·∂ø = softmax(logits; dims=1)
# #                 logœÄ·∂ø = logsoftmax(logits; dims=1)
# #                 logœÄ·∂ø‚Çê = [
# #                     logœÄ·∂ø[batch·µÉ[i], i]
# #                     for i in 1:agent.params.minibatch_sizes[1]
# #                 ]
# #                 ratio = @. exp(logœÄ·∂ø‚Çê - logœÄ·µíÀ°·µà‚Çê)
# #                 L = vec(batch·¥¨) .* ratio
# #                 g = @. (
# #                     batch·¥¨ * clamp(ratio, 1 - agent.params.ùúÄ, 1 + agent.params.ùúÄ)
# #                 )
# #                 policy_loss = -mean(min.(L, g))
# #                 entropy = -mean(sum(œÄ·∂ø .* logœÄ·∂ø; dims=1))
# #                 entropy_bonus = -agent.params.Œ± * entropy
# #                 ignore() do
# #                     losses[1, i] += policy_loss
# #                     losses[2, i] += entropy
# #                     losses[3, i] += entropy_bonus
# #                 end
# #                 return policy_loss + entropy_bonus
# #             end
# #             update!(agent.opt_states[1], agent.networks.actor_critic, ‚àá[1])
# #         end
# #         losses[:, i] ./= agent.params.capacity √∑ agent.params.minibatch_sizes[1]
# #     end
# #     return vec(mean(losses; dims=2))
# # end

# # function _update_agent_distillation!(
# #     agent::PPOAgent{T, PT, R, CircularReplayBuffer{T, Int}}
# # ) where {T, R, PT}
# #     losses = zeros(T, 2, agent.params.training_steps[3])

# #     œÄÃÉ = get_œÄÃÉ(agent.networks, agent.memory.observations)
# #     _update_logœÄ·µíÀ°·µà!(agent, œÄÃÉ)
# #     for i in 1:agent.params.training_steps[3]
# #         for (batch·µí, batch·µíÀ°·µà) in DataLoader(
# #             (agent.memory.observations[:, 1 : end - 1], agent.logœÄ·µíÀ°·µà);
# #             batchsize=agent.params.minibatch_sizes[3],
# #         )
# #             values·µ• = agent.networks.value_layers(batch·µí)
# #             ‚àá = gradient(agent.networks.actor_critic) do m
# #                 logits, values‚Çö = m(batch·µí)
# #                 logœÄ·∂ø = logsoftmax(logits; dims=1)
# #                 kl_divergence = agent.params.Œ≤ * mean(
# #                     sum(@. exp(batch·µíÀ°·µà) * (batch·µíÀ°·µà - logœÄ·∂ø); dims=1)
# #                 )
# #                 value_distillation = mse(values‚Çö, values·µ•)
# #                 ignore() do
# #                     losses[1, i] += value_distillation
# #                     losses[2, i] += kl_divergence
# #                 end
# #                 return value_distillation + kl_divergence
# #             end
# #             update!(agent.opt_states[3], agent.networks.actor_critic, ‚àá[1])
# #         end
# #         losses[:, i] ./= agent.params.capacity √∑ agent.params.minibatch_sizes[3]
# #     end
# #     return vec(mean(losses; dims=2))
# # end

# ##################
# # Agent learning #
# ##################
# function learn!(agent::PPOAgent, env::QuantumControlEnvironment)
#     rewards = zeros(eltype(env), agent.params.epochs)
#     dones = zeros(Int, agent.params.epochs)
#     losses = zeros(Float32, 6, agent.params.epochs)

#     _initial_steps(agent, env)

#     for epoch in 1:agent.params.epochs
#         epoch·µ£ = evaluation_steps!(agent, env)
#         epoch‚Çó = trainer_steps!(agent)

#         rewards[epoch] = epoch·µ£
#         dones[epoch] = sum(agent.memory.dones)
#         losses[:, epoch] = epoch‚Çó

#         println(
#             "Epoch: ",
#             epoch,
#             "| Terminations: ",
#             dones[epoch],
#             "| Average Episodic Reward: ",
#             rewards[epoch] / dones[epoch],
#         )
#     end
#     return rewards ./ dones, losses
# end
